## Scheduler Kubernetes Intelligent (RL-DQN) pour le Network Slicing 5G

**Projet d'Infrastructure Intelligente Logicielle des RÃ©seaux Mobiles - 2025**

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://python.org)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-1.31+-blue.svg)](https://kubernetes.io)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## Auteurs
**Colson Eliott** | **ABDERRAHMANE Sonia** | **Garnaud ThÃ©o**

---

## Table des MatiÃ¨res
1. [Ã‰tat de lâ€™Art](#1-Ã©tat-de-lart--le-verrou-technologique)
2. [MÃ©thodologie et Justifications Techniques](#2-mÃ©thodologie-et-justifications-techniques)
3. [RÃ©sultats AcadÃ©miques](#3-rÃ©sultats-acadÃ©miques)
4. [Guide de Reproduction (Installation & Tests)](#4-guide-de-reproduction-installation--tests)
5. [Structure du Projet](#5-structure-du-projet)

---
## Ã‰tat de lâ€™Art
## Contexte et ProblÃ©matique

L'architecture 5G repose sur le Network Slicing, une technologie permettant de crÃ©er des rÃ©seaux virtuels adaptÃ©s Ã  des besoins spÃ©cifiques : URLLC (Ultra-Reliable Low Latency Communications) pour les applications critiques et eMBB (Enhanced Mobile Broadband) pour le haut dÃ©bit. Dans ce contexte, les fonctions rÃ©seau (CNF) comme l'UPF (User Plane Function) sont conteneurisÃ©es et orchestrÃ©es par Kubernetes.

Le dÃ©fi majeur rÃ©side dans le placement optimal de ces conteneurs sur des clusters Edge hÃ©tÃ©rogÃ¨nes. Un mauvais placement entraÃ®ne une violation des SLA (Service Level Agreements), notamment une latence trop Ã©levÃ©e pour les cas d'usage critiques (vÃ©hicules autonomes, industrie 4.0).

## Limites de l'Ordonnanceur Natif (Kube-Scheduler)
Le scheduler par dÃ©faut de Kubernetes, a Ã©tÃ© conÃ§u pour des applications web gÃ©nÃ©riques et non pour les contraintes topologiques strictes des rÃ©seaux tÃ©lÃ©coms. Son fonctionnement se dÃ©compose en deux phases sÃ©quentielles :
* Le Filtrage (Predicates) : Ã‰limination des nÅ“uds ne rÃ©pondant pas aux exigences hard (CPU/RAM insuffisants, ports indisponibles).
* Le Scoring (Priorities) : Classement des nÅ“uds restants selon des fonctions de score statiques, telles que LeastRequestedPriority (favoriser les nÅ“uds les moins chargÃ©s) ou NodeAffinity.

Bien que robuste, cette approche prÃ©sente des lacunes structurelles pour la 5G :

* CÃ©citÃ© Topologique : Le scheduler considÃ¨re le cluster comme un ensemble "plat". Il ne modÃ©lise pas nativement la latence rÃ©seau inter-nÅ“uds, ce qui peut conduire Ã  placer un UPF critique sur un nÅ“ud gÃ©ographiquement distant de l'utilisateur (UE), augmentant ainsi la latence de bout en bout.
* Approche RÃ©active et "Gloutonne" : Les dÃ©cisions sont prises pod par pod, sans vision globale ni anticipation de la charge future. Comme le soulignent Jian et al., cette approche locale peut entraÃ®ner une fragmentation des ressources et un dÃ©sÃ©quilibre de charge (load imbalance) Ã  l'Ã©chelle du cluster, nuisant Ã  la performance globale.


## Approches par Apprentissage par Renforcement (DRL)
Pour pallier la rigiditÃ© des rÃ¨gles statiques, la littÃ©rature rÃ©cente propose l'utilisation du Deep Reinforcement Learning (DRL). Cette mÃ©thode permet Ã  un agent d'apprendre une politique de placement optimale par "essais-erreurs" en interagissant avec l'environnement Kubernetes.

#### Optimisation centrÃ©e sur la Latence (Algorithme PPO) :
Wang et al. (2023) adressent spÃ©cifiquement la problÃ©matique de la latence dans les clusters Edge via l'algorithme PPO-LRT (Proximal Policy Optimization with Least Response Time).
  - MÃ©thode : Ils modÃ©lisent le processus de scheduling comme un Processus de DÃ©cision Markovien (MDP) oÃ¹ la fonction de rÃ©compense intÃ¨gre directement le temps de rÃ©ponse (LRT).
  - RÃ©sultats : Leur approche dÃ©montre une rÃ©duction de 31% du temps de rÃ©ponse moyen par rapport au kube-scheduler natif et une meilleure rÃ©partition de la charge lors des pics de trafic.
  - Limitation : Bien que performant, l'algorithme PPO (Policy Gradient) peut s'avÃ©rer instable lors de l'entraÃ®nement et complexe Ã  converger dans des environnements trÃ¨s dynamiques.

#### Optimisation centrÃ©e sur les Ressources (Algorithme DQN) :
Jian et al. (2024) proposent une approche diffÃ©rente avec le systÃ¨me DRS (Deep Reinforcement Learning Scheduler), basÃ© sur l'algorithme DQN (Deep Q-Network).
  * MÃ©thode : Leur modÃ¨le se concentre sur une vision globale des ressources (CPU, MÃ©moire, RÃ©seau, Disque) pour minimiser la variance de charge entre les nÅ“uds. Ils introduisent un moniteur spÃ©cifique pour percevoir l'Ã©tat global du cluster.
  * RÃ©sultats : DRS amÃ©liore l'utilisation des ressources de 27.29% et rÃ©duit le dÃ©sÃ©quilibre de charge d'un facteur 2.90x par rapport Ã  la solution native.
  * Limitation : Cette approche excelle pour l'efficacitÃ© Ã©nergÃ©tique et la densitÃ© (eMBB), mais ne priorise pas explicitement la contrainte de latence critique pour les services URLLC.


## Conclusion :
L'analyse de la littÃ©rature scientifique et technique met en Ã©vidence une limitation critique dans les infrastructures actuelles pour le Network Slicing 5G. D'une part, l'ordonnanceur natif (Kube-Scheduler) se rÃ©vÃ¨le inadaptÃ© aux exigences topologiques de la 5G en raison de son approche statique et de sa "cÃ©citÃ©" rÃ©seau. D'autre part, les solutions basÃ©es sur l'IA se divisent en deux camps distincts qui ne communiquent pas. Soit les approches se focalisent exclusivement sur la latence (ex: PPO de Wang et al.), souvent instables Ã  entraÃ®ner. Ou bien, les approches focalisÃ©es exclusivement sur l'Ã©quilibrage de charge (ex: DRS de Jian et al.), qui nÃ©gligent la proximitÃ© critique pour l'URLLC. Ainsi, il n'existe donc pas, Ã  l'heure actuelle, de solution unifiÃ©e capable de satisfaire simultanÃ©ment les contraintes contradictoires de l'URLLC (latence) et de l'eMBB (charge) sur des architectures Edge hÃ©tÃ©rogÃ¨nes. C'est ce verrou technologique que notre projet se propose de lever. Nous formulons l'hypothÃ¨se qu'une architecture hybride, utilisant la stabilitÃ© de l'algorithme DQN (validÃ©e par Jian et al.) mais guidÃ©e par une fonction de rÃ©compense sensible Ã  la latence (inspirÃ©e de Wang et al.), permettra d'atteindre ce compromis optimal. De plus, pour garantir la validitÃ© de ces rÃ©sultats face Ã  l'hÃ©tÃ©rogÃ©nÃ©itÃ© matÃ©rielle identifiÃ©e lors de nos tests prÃ©liminaires (Mac/PC), cette approche doit impÃ©rativement Ãªtre validÃ©e sur une infrastructure conteneurisÃ©e agnostique (Docker/k3d).

---

## 2. MÃ©thodologie et Justifications Techniques

Pour rÃ©pondre Ã  l'objectif de minimisation de la latence, nous avons dÃ©veloppÃ© une architecture logicielle spÃ©cifique, qui est justifiÃ©e par les contraintes de simulation et de reproductibilitÃ©.

#### 2.1. Infrastructure de Simulation : Le Choix de la Conteneurisation (k3d)
Lors de nos travaux prÃ©liminaires, l'utilisation de machines virtuelles s'est avÃ©rÃ©e complexe Ã  cause de l'architecture diffÃ©rentes de nos processeurs (Mac/PC).
* Solution : Migration vers une architecture conteneurisÃ©e avec k3d (Kubernetes dans Docker).
* PrÃ©cision : L'environnement nexslice dans nos scripts correspond ici Ã  un cluster k3d local simulant une topologie Edge.
* Justification : Cela garantit la reproductibilitÃ© des rÃ©sultats. Et cela permet de simuler un cluster hÃ©tÃ©rogÃ¨ne sur une seule machine physique.

#### 2.2 Algorithme de DÃ©cision : Deep Q-Network (DQN)
Nous avons implÃ©mentÃ© un agent RL-DQN plutÃ´t qu'une heuristique figÃ©e.
* Justification : Le DQN permet d'apprendre une politique de placement dynamique. L'agent reÃ§oit un Ã©tat simplifiÃ© du cluster et apprend Ã  identifier le nÅ“ud optimal via un rÃ©seau de neurones Ã  3 couches (Input 7 -> 64 -> 32 -> Output 1). Contrairement Ã  une simple table Q-Learning, le rÃ©seau de neurones permet de gÃ©nÃ©raliser l'apprentissage Ã  des Ã©tats non rencontrÃ©s.

#### 2.3 StratÃ©gie de Placement : Latence par Identification Topologique
Contrairement aux approches basÃ©es sur des mÃ©triques temps rÃ©el (souvent bruitÃ©es ou difficiles Ã  collecter sans une stack de monitoring lourde type Prometheus), nous avons optÃ© pour une stratÃ©gie d'apprentissage topologique dÃ©terministe, implÃ©mentÃ©e directement dans l'environnement RL (rl_environment.py).

**1- DÃ©tection Topologique par Nom de NÅ“ud :** PlutÃ´t que de s'appuyer sur des labels Kubernetes dynamiques, notre environnement identifie les nÅ“uds Ã  faible latence (Edge) via une convention de nommage explicite sur le nom d'hÃ´te.
  * agent-0 : IdentifiÃ© comme le nÅ“ud Low-Latency (proximitÃ© utilisateur).
  * agent-1 (et autres) : IdentifiÃ©s comme nÅ“uds Standard (Cloud).
  * Justification : Cette mÃ©thode offre une stabilitÃ© absolue Ã  l'agent RL pour distinguer la topologie du rÃ©seau dans un environnement de simulation local.

**2- Abstraction des Ressources CPU :** Pour cette preuve de concept focalisÃ©e sur la latence, l'agent ignore volontairement la charge CPU (valeur fixÃ©e Ã  0.0 dans l'Ã©tat observÃ©) pour concentrer l'apprentissage exclusivement sur la topologie rÃ©seau.

**3- Fonction de RÃ©compense Binaire :** Pour forcer la convergence vers le nÅ“ud Edge, nous avons dÃ©fini une fonction de rÃ©compense binaire ("Sparse Reward"). L'agent reÃ§oit une rÃ©compense massive uniquement s'il cible le bon nÅ“ud gÃ©ographique :

$$
R = \begin{cases} 
100.0 & \text{si nÅ“ud Low-Latency (Agent-0)} \\
10.0 & \text{si nÅ“ud Standard (Agent-1)}
\end{cases}
$$

Cette diffÃ©rence massive de reward (x10) permet Ã  l'agent de converger trÃ¨s rapidement vers la solution optimale (le nÅ“ud Edge) tout en laissant le filtre de sÃ©curitÃ© gÃ©rer les cas de saturation.

---

## 3. RÃ©sultats AcadÃ©miques

Les benchmarks ont Ã©tÃ© rÃ©alisÃ©s sur un cluster de 2 nÅ“uds (1 Edge Low-Latency, 1 Standard) avec 10 rÃ©plicas de pods UPF.

#### Performance de Latence (Cas URLLC)

| Solution | Latence P95 (Est.) | Gain | Observation |
| :--- | :--- | :--- | :--- |
| **Baseline (Kube-Scheduler)** | ~ 30 ms | - | Placement alÃ©atoire (~40% sur nÅ“ud rapide). |
| **Notre Agent RL** | ~ 10 ms | ~ -66% | Consolidation intelligente sur le nÅ“ud Edge. |


#### Visualisation des DonnÃ©es

**Gain de Latence (URLLC)**
![Latence](TESTS/latency_p95.png)
*L'agent RL (vert) rÃ©duit drastiquement la latence P95.*

---

## 4. Guide de Reproduction (Installation & Tests)

#### PrÃ©-requis
* Linux, macOS ou Windows (WSL2).
* Droits `sudo` (pour Docker).

#### Installation Automatique (via un script)


<details>
<summary><strong> Option A : utilisation d'un script </strong></summary>

Copiez-collez simplement ce bloc dans votre terminal pour tout installer et lancer :

```bash
# 1. CrÃ©er le script d'installation
cat << 'EOF' > install_project.sh
#!/bin/bash
set -e
echo -e "\nğŸ”µ --- INSTALLATION AUTOMATISÃ‰E DU SCHEDULER IA ---"

# Installation des dÃ©pendances (Linux/Debian/Ubuntu)
if [ -f /etc/debian_version ]; then
    echo "ğŸ”§ Installation des prÃ©-requis systÃ¨me..."
    sudo apt-get update -q
    sudo apt-get install -y curl git docker.io python3 python3-pip python3-venv jq
    if ! sudo service docker status > /dev/null 2>&1; then sudo service docker start; fi
    sudo chmod 666 /var/run/docker.sock
fi

# Installation K3D et Kubectl
if ! command -v k3d &> /dev/null; then
    curl -s [https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh](https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh) | bash
fi
if ! command -v kubectl &> /dev/null; then
    curl -LO "[https://dl.k8s.io/release/$(curl](https://dl.k8s.io/release/$(curl) -L -s [https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl](https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl)"
    sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
fi

# Mise en place du projet
cd ~
if [ -d "kubernetes-ia-scheduler" ]; then rm -rf kubernetes-ia-scheduler; fi
git clone [https://github.com/sohooow/kubernetes-ia-scheduler.git](https://github.com/sohooow/kubernetes-ia-scheduler.git)
cd kubernetes-ia-scheduler

# Environnement Python
echo "ğŸ Configuration Python..."
python3 -m venv .venv
source .venv/bin/activate
pip install -r configuration/requirements.txt

# Cluster Kubernetes
echo "ğŸ—ï¸  CrÃ©ation du cluster 'nexslice'..."
k3d cluster delete nexslice 2>/dev/null || true
k3d cluster create nexslice --agents 2 --wait
# Labelisation pour la topologie
kubectl label node k3d-nexslice-agent-0 type=low-latency --overwrite
kubectl label node k3d-nexslice-agent-1 type=standard --overwrite

# DÃ©ploiement
echo "ğŸš€ DÃ©ploiement du Scheduler..."
kubectl apply -f kubernetes/ia-scheduler-deploy.yaml

echo -e "\nâœ… INSTALLATION TERMINÃ‰E."
echo "ğŸ‘‰ Lancement des tests acadÃ©miques..."
chmod +x TESTS/test_academic_scenarios.sh
./TESTS/test_academic_scenarios.sh

echo -e "\nğŸ“Š GÃ©nÃ©ration des graphiques..."
python3 TESTS/generate_academic_plots.py
echo "Les rÃ©sultats sont disponibles dans le dossier TESTS/RESULTS/"
EOF

# 2. Rendre exÃ©cutable et lancer
chmod +x install_project.sh
./install_project.sh
```
</details>

#### Option B : Installation Manuelle (Pas Ã  pas)
  
#### Installation rapide (macOS / Linux)

#### MacOS (via Homebrew)
```bash
brew install k3d kubectl python jq
```
#### Linux (Ubuntu/Debian)
```bash
# Installer curl et docker
sudo apt update && sudo apt install -y curl git
sudo apt-get update && sudo apt-get install -y docker.io python3 python3-pip jq

# Installer k3d
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash

# Installer kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# Valider l'installation de kubectl
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

#### DÃ©marrage Rapide

#### 1\. Installation et DÃ©ploiement

```bash
# Cloner le repository
git clone https://github.com/sohooow/kubernetes-ia-scheduler.git
cd kubernetes-ia-scheduler

# CrÃ©er et activer l'environnement virtuel (Indispensable)
python3 -m venv .venv
source .venv/bin/activate

# Installer les dÃ©pendances
pip install -r configuration/requirements.txt

# CrÃ©er cluster k3d et labelliser
k3d cluster create nexslice --agents 2
kubectl label node k3d-nexslice-agent-0 type=low-latency
kubectl label node k3d-nexslice-agent-1 type=standard

# DÃ©ployer le scheduler avec RBAC
kubectl apply -f kubernetes/ia-scheduler-deploy.yaml
```

#### 2\. ExÃ©cution des Tests AcadÃ©miques

```bash
# Lancer la suite de tests complÃ¨te (Baseline, EL)
# Ceci exÃ©cute les 2 scÃ©narios et produit le rapport d'analyse.
./TESTS/test_academic_scenarios.sh
```

**Sortie attendue** :

<details>
<summary>Cliquez ici pour voir la sortie complÃ¨te du terminal (Logs de succÃ¨s)</summary>

```text
(.venv) TSP@MBAEliott kubernetes-ia-scheduler % ./TESTS/test_academic_scenarios.sh
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   TESTS ACADÃ‰MIQUES - SCHEDULER RL pour 5G Network Slicing     â•‘
â•‘   Politiques: Baseline         |               EL (Latency)    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Suppression de l'ancien fichier academic_results.json...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TEST BASELINE : kube-scheduler (Politique par dÃ©faut)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Nettoyage des dÃ©ploiements...
deployment.apps/test-baseline created
Attente du scheduling (30s)...
Distribution:
   Worker-1 (low-latency / k3d-nexslice-agent-0): 4 pods
   Worker-2 (standard / k3d-nexslice-agent-1):    3 pods
   Autres (Master/Server):                   3 pods
   Running: 10/10, Pending: 0
MÃ©triques:
   Latence P95: 27.14 ms
   Variance CPU: .50
Test Baseline terminÃ©

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TEST 1 (EL) : Politique PrioritÃ© Latence (Edge-Latency)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Nettoyage des dÃ©ploiements...
deployment.apps "test-baseline" deleted from default namespace
DÃ©marrage Scheduler RL (mode EL)...
  Scheduler PID: 82228 (Mode: Latency)
deployment.apps/test-el-latency created
Attente du scheduling (40s)...
Distribution:
   Worker-1 (low-latency / k3d-nexslice-agent-0): 10 pods
   Worker-2 (standard / k3d-nexslice-agent-1):    0 pods
   Autres (Master/Server):                   0 pods
   Running: 10/10, Pending: 0
MÃ©triques:
   Latence P95: 10.00 ms
   Variance CPU: 50.00
./TESTS/test_academic_scenarios.sh: line 211: 82228 Terminated: 15          python -m schedulers.ia_scheduler_rl > /tmp/scheduler_el.log 2>&1
Test EL (Latency) terminÃ©

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SynthÃ¨se des rÃ©sultats
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DEBUG: Valeurs capturÃ©es pour le JSON :
  EL_W1: 10, EL_W2: 0
Fichier JSON mis Ã  jour avec succÃ¨s : -rw-r--r--@ 1 TSP  staff  794 Nov 28 23:18 academic_results.json

Tests terminÃ©s. VÃ©rifiez academic_results.json

Nettoyage des dÃ©ploiements...
deployment.apps "stress-load" deleted from default namespace
```

</details>
 
### RÃ©sultats
Une fois les tests rÃ©alisÃ©s avec succÃ¨s, lancez la commande suivante pour crÃ©er des rÃ©sultats visuels sous forme de graphiques :

```bash
python3 ./TESTS/generate_academic_plots.py
```

Les graphiques sont sauvegardÃ©s dans ```/TESTS/RESULTS```

---

## 5. Structure du Projet
```
â”œâ”€â”€ configuration/            # DÃ©pendances et Dockerfile
â”œâ”€â”€ kubernetes/               # Manifestes YAML (Deployment, RBAC, Pods de test)
â”œâ”€â”€ schedulers/               # Code source Python de l'IA
â”‚   â”œâ”€â”€ ia_scheduler_rl.py    # Point d'entrÃ©e du Scheduler
â”‚   â”œâ”€â”€ rl_agent.py           # RÃ©seau de neurones (DQN)
â”‚   â”œâ”€â”€ rl_environment.py     # Environnement et Fonction de RÃ©compense
â”‚   â””â”€â”€ scoring_logic.py      # Logique de scoring
â”œâ”€â”€ TESTS/                    # Scripts de validation scientifique
â”‚   â”œâ”€â”€ test_academic_scenarios.sh   # Script principal de test
â”‚   â”œâ”€â”€ generate_academic_plots.py   # GÃ©nÃ©ration des graphiques
â”‚   â””â”€â”€ RESULTS/              # Graphiques gÃ©nÃ©rÃ©s
â”œâ”€â”€ rl_scheduler_model.pth    # ModÃ¨le IA prÃ©-entraÃ®nÃ©
â””â”€â”€ README.md                 # Ce fichier
```
