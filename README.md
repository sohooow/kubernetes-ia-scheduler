# kubernetes-ia-scheduler

**Scheduler Kubernetes personnalis√© (IA)** qui s√©lectionne intelligemment les n≈ìuds en fonction d'une heuristique pond√©r√©e combinant **latence** et **charge CPU**.

## üéØ Fonctionnalit√©s

- **Scoring intelligent** : algorithme pond√©r√© `Score = W_L √ó (1/latence) + W_U √ó (1/(1+CPU))`
- **Poids configurables** : ajustez `W_L` (latence) et `W_U` (charge) selon vos besoins
- **D√©ploiement flexible** : fonctionne en local (Python) ou dans le cluster (conteneuris√©)
- **Fallback robuste** : binding standard + patch fallback pour compatibilit√©
- **RBAC complet** : permissions Kubernetes configur√©es pour production

## üìÅ Structure du projet

```
‚îú‚îÄ‚îÄ ia_scheduler.py          # Scheduler principal (watch + placement)
‚îú‚îÄ‚îÄ scoring_logic.py          # Logique de scoring avec poids W_L/W_U
‚îú‚îÄ‚îÄ Dockerfile                # Image Docker du scheduler
‚îú‚îÄ‚îÄ ia-scheduler-deploy.yaml  # Manifest K8s (RBAC + Deployment)
‚îú‚îÄ‚îÄ requirements.txt          # D√©pendances Python
‚îú‚îÄ‚îÄ upf-pod-*.yaml           # Manifests de test
‚îî‚îÄ‚îÄ stress-base-load.yaml    # Charge de travail pour tests
```

## üöÄ D√©ploiement

### Option 1 : D√©ploiement dans Kubernetes (Recommand√©)

#### √âtape 1 : Cr√©er/configurer le cluster

**Avec k3d (NexSlice) :**
```bash
# Cr√©er un cluster k3d avec 3 n≈ìuds
k3d cluster create nexslice-cluster
k3d node create nexslice-worker-1 --cluster nexslice-cluster --role agent
k3d node create nexslice-worker-2 --cluster nexslice-cluster --role agent

# Labelliser les n≈ìuds pour simuler diff√©rentes latences
kubectl label node k3d-nexslice-worker-1-0 type=low-latency
kubectl label node k3d-nexslice-worker-2-0 type=standard
```

**Avec kind :**
```bash
# Cr√©er un cluster kind multi-n≈ìuds
kind create cluster --config kind-config.yaml

# Labelliser les n≈ìuds
kubectl label node kind-worker type=low-latency
kubectl label node kind-worker2 type=standard
```

#### √âtape 2 : D√©ployer le scheduler

```bash
# D√©ployer le scheduler avec RBAC
kubectl apply -f ia-scheduler-deploy.yaml

# V√©rifier le d√©ploiement
kubectl get pods -l app=custom-ia-scheduler
kubectl logs deployment/ia-scheduler-deployment
```

#### √âtape 3 : Tester avec des pods

```bash
# D√©ployer des pods utilisant le scheduler IA
kubectl apply -f upf-pod-ia-L.yaml

# Observer la distribution
kubectl get pods -o wide

# Voir les d√©cisions du scheduler
kubectl logs -f deployment/ia-scheduler-deployment
```

### Option 2 : Ex√©cution locale (D√©veloppement)

```bash
# 1. Cr√©er/activer un virtualenv
python -m venv .venv
source .venv/bin/activate

# 2. Installer les d√©pendances
pip install -r requirements.txt

# 3. Lancer le scheduler localement
python ia_scheduler.py
```

## üîß Configuration

### Ajuster les poids du scheduler

√âditez `scoring_logic.py` :

```python
# Sc√©nario 1 : Privil√©gier la latence (fonctions 5G critiques)
W_L = 0.8  # Poids latence
W_U = 0.2  # Poids charge
CPU_THRESHOLD = 2.0  # Seuil CPU pour p√©nalit√©

# Sc√©nario 2 : Privil√©gier l'√©quilibrage de charge
W_L = 0.2
W_U = 0.8
```

**Algorithme de scoring am√©lior√© :**
```python
L_score = (50.0 / L_node) ** 2  # Formule quadratique pour amplifier les diff√©rences
U_score = 1.0/(1.0+U_cpu) if U_cpu < CPU_THRESHOLD else 0.1/(1.0+U_cpu)  # P√©nalit√© pour surcharge
score = (W_L * L_score) + (W_U * U_score)
```

Puis reconstruisez l'image Docker :
```bash
docker build -t soohow/ia-scheduler:latest .
docker push soohow/ia-scheduler:latest
kubectl rollout restart deployment ia-scheduler-deployment
```

### Configurer Prometheus (optionnel)

Pour obtenir les m√©triques CPU r√©elles via Prometheus :

```bash
# Installer Prometheus dans le cluster
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/bundle.yaml

# Port-forward pour acc√®s local (si scheduler tourne localement)
kubectl port-forward -n monitoring svc/prometheus 9090:9090
```

Si Prometheus n'est pas disponible, le scheduler utilise automatiquement un fallback bas√© sur la somme des `requests` CPU des pods.

## üìä R√©sultats de tests

### Test avec W_L=0.8 (priorit√© latence) - Algorithme am√©lior√©

**Distribution observ√©e sur cluster NexSlice (3 n≈ìuds, 5 replicas) :**

| N≈ìud | Type | Latence simul√©e | Pods d√©ploy√©s | Score moyen |
|------|------|----------------|---------------|-------------|
| **k3d-nexslice-worker-1-0** | low-latency | 10ms | **5 pods** ‚úÖ | **~20.14** |
| k3d-nexslice-worker-2-0 | standard | 50ms | 0 pods | ~0.97 |
| k3d-nexslice-cluster-server-0 | control-plane | 50ms | 0 pods | ~0.94 |

**R√©sultat :** Avec la formule quadratique `(50.0/latency)¬≤`, le scheduler respecte parfaitement la pond√©ration W_L=0.8 en cr√©ant un **√©cart de score de ~20x** entre les n≈ìuds low-latency et standard. Tous les 5 pods sont plac√©s sur le n≈ìud √† faible latence comme attendu.

**Latence P95 estim√©e :** **10ms** (100% des pods sur n≈ìud low-latency)

## üê≥ Image Docker

Image publique disponible sur Docker Hub :
```bash
docker pull soohow/ia-scheduler:latest
```

## üìù Notes importantes

- **schedulerName** : Les pods doivent sp√©cifier `schedulerName: custom-ia-scheduler` pour √™tre trait√©s par ce scheduler
- **Coexistence** : Peut tourner en parall√®le du scheduler par d√©faut de Kubernetes
- **Permissions RBAC** : Le manifest `ia-scheduler-deploy.yaml` configure automatiquement les permissions n√©cessaires
- **Metrics API** : Si `kubectl top` renvoie une erreur, installez `metrics-server` dans votre cluster

## üîó Liens

- **GitHub** : https://github.com/sohooow/kubernetes-ia-scheduler
- **Docker Hub** : https://hub.docker.com/r/soohow/ia-scheduler

## üìÑ Licence

MIT (√† ajouter si besoin)
